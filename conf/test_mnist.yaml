# @package _global_

defaults:
  - _self_
  - launcher: slurm_gpu
  - test_job
  - dataset: mnist_superres #celeba, 2d, stackedmnist
  - model: SuperResUNET #Basic, UNET
  - override hydra/launcher: submitit_slurm

name: ${Dataset}_${data.dataset}
run: 0
nosave: False

# logging
LOGGER: Wandb  # CSV, Wandb, NONE
CSV_log_dir: ./

# training
optimizer: Adam
cache_cpu: True
test_batch_size: 200
plot_level: 3
mean_match: True
paths:
  experiments_dir_name: experiments
  data_dir_name: data

symmetric_gamma: False
var_final_gamma_scale: False
double_gamma_scale: True
langevin_scale: 2*torch.sqrt(gamma)

# checkpoint
checkpoint_run: True
checkpoint_it: 1
checkpoint_pass: b  # b or f (skip b ipf run)
checkpoint_iter: 500000
checkpoint_dir: experiments/stackedmnist_superres_4/2022-02-04/cfg-cond_final=True,final_adaptive=True,gamma_max=0.05,num_cache_batches=4,num_iter=500000,num_steps=10/00-34-20
sample_checkpoint_f: null  # ${checkpoint_dir}/checkpoints/version_0/sample_net_f_9_50000.ckpt
sample_checkpoint_b: ${checkpoint_dir}/checkpoints/version_0/sample_net_b_1_500000.ckpt
checkpoint_f: null  # ${checkpoint_dir}/checkpoints/version_0/net_f_9_50000.ckpt
checkpoint_b: ${checkpoint_dir}/checkpoints/version_0/net_b_1_500000.ckpt
optimizer_checkpoint_f: null  # ${checkpoint_dir}/checkpoints/version_0/optimizer_f_9_50000.ckpt
optimizer_checkpoint_b: ${checkpoint_dir}/checkpoints/version_0/optimizer_b_1_500000.ckpt
